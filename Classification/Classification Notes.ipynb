{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Notes\n",
    "\n",
    "Here I'll work through my reasoning from choosing features to training a model on the data.\n",
    "\n",
    "Possible features:\n",
    "\n",
    "- crowLength\n",
    "- pathCrowRatio\n",
    "- coveredArea\n",
    "- windowArea\n",
    "- areaPerUnitL\n",
    "- areaPerUnitT\n",
    "- hurst\n",
    "- DFA\n",
    "- angleDensS\n",
    "- angleDensT\n",
    "- timeSpent\n",
    "- corrDim\n",
    "\n",
    "Possible labels:\n",
    "\n",
    "- transMode\n",
    "\n",
    "Could try and use some of the features as labels instead?\n",
    "\n",
    "Ideally I would put all of these into a table with each row representing a trajectory, this might be very large though - let's see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Metadata/Inventory.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13395"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have roughly 13,400 trajectories, this is manageable. Now we need to write a script which will populate a csv with the features and labels for each trajectory. This may take a while.\n",
    "\n",
    "I've discarded the DFA feature because it was too buggy and I'm not sure how much it really adds.\n",
    "\n",
    "The featExtract script seems to be working nicely, giving me the csv I need. I've had to filter (arbitrarily) to trajectories with no fewer than 20 points, that are of 0.5mins < duration < 60mins and are longer than 20m. This was on one hand to remove the noisy short trajectories and also reduce computation time on the longer trajectories (the latter could be relaxed with access to more power).\n",
    "\n",
    "I have decided to remove the correlation dimension feature as I was unsure that it was valid on this time-series data and it also created a large computational burden. Random Forest analysis on a small dataset showed it also to be a weak predictor of mode of transport. I've also increased the efficiency of the angle-density measures.\n",
    "\n",
    "I have converted the trajectories into 32x32 histograms, I intend to use these with a CNN to see if I get any interesting results. My guess is that it will perform poorly but it'll be interesting to see.\n",
    "\n",
    "The updated list of features now reads:\n",
    "\n",
    "- crowLength\n",
    "- pathCrowRatio\n",
    "- coveredArea\n",
    "- windowArea\n",
    "- areaPerUnitL\n",
    "- areaPerUnitT\n",
    "- hurst\n",
    "- angleDensS\n",
    "- angleDensT\n",
    "- timeSpent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
